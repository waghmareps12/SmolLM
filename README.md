# ğŸš€ Small Language Model with LoRA Fine-Tuning

This project implements a **500M parameter language model** fine-tuned with **LoRA (Low-Rank Adaptation)** for efficient training on consumer hardware. It includes training, inference, and dataset preprocessing.

## ğŸ“‚ Project Structure
```
â”‚â”€â”€ model.py        # Defines the Transformer-based language model
â”‚â”€â”€ train.py        # Trains the model using LoRA
â”‚â”€â”€ inference.py    # Generates text from the trained model
â”‚â”€â”€ requirements.txt # Required dependencies
â”‚â”€â”€ README.md       # Project documentation
```

## ğŸ“Œ Features
- **Lightweight Transformer Model**
- **LoRA Fine-Tuning** for reduced GPU memory usage
- **Autoregressive Text Generation**
- **Wikitext-2 Dataset Preprocessing**

## ğŸ›  Installation
First, install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ¯ Training the Model
Run the training script to fine-tune the model with LoRA:
```bash
python train.py
```
This will:
- Load a custom Transformer model.
- Apply LoRA to reduce trainable parameters.
- Train on the Wikitext-2 dataset.
- Save LoRA adapters in `lora_adapter/`.

## ğŸ“ Inference (Text Generation)
After training, generate text with:
```bash
python inference.py
```
Example output:
```
Once upon a time, in a faraway kingdom...
```

## ğŸ— Model Details
- **Transformer Architecture:** 6 layers, 8 heads
- **Embedding Size:** 512
- **LoRA Rank:** 8
- **Dataset:** Wikitext-2

## ğŸ“Œ LoRA Benefits
âœ… **Reduces GPU memory** usage
âœ… **Faster fine-tuning**
âœ… **Lightweight adaptation**

## ğŸ“œ License
This project is licensed under the MIT License.

---
ğŸš€ Happy Fine-Tuning! Let me know if you need any improvements. ğŸ¯


