# ğŸš€ Small Language Model Training

This project implements a **125M parameter language model** optimized for training on consumer hardware with limited VRAM (4GB+). It includes efficient training with gradient accumulation and length-based batch scheduling.

## ğŸ“‚ Project Structure
```
â”‚â”€â”€ model.py        # Transformer-based language model (125M params)
â”‚â”€â”€ train.py        # Training script with memory optimizations
â”‚â”€â”€ inference.py    # Text generation script
â”‚â”€â”€ requirements.txt # Required dependencies
â”‚â”€â”€ README.md       # Project documentation
```

## ğŸ“Œ Features
- **Memory-Efficient Transformer Model** (~125M parameters)
- **Length-Based Batch Scheduling** for efficient training
- **Gradient Accumulation** for effective larger batch sizes
- **Autoregressive Text Generation**
- **Wikitext-2 Dataset Integration**

## ğŸ›  Installation
Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ¯ Training the Model
Run the training script:
```bash
python train.py
```

The training process includes:
- Automatic GPU/CPU device selection
- Dynamic batch scheduling by sequence length
- Gradient accumulation (effective batch size: 16)
- Automatic checkpointing
- Cosine learning rate scheduling

## ğŸ“ Inference
Generate text using the trained model:
```bash
python inference.py
```

## ğŸ— Model Architecture
- **Layers:** 12 transformer blocks
- **Attention Heads:** 12 heads
- **Embedding Dimension:** 768
- **Context Window:** 512 tokens
- **Total Parameters:** ~125M
- **Activation:** GELU
- **Layer Normalization:** Pre-norm architecture

## âš¡ Performance Optimizations
- âœ… Length-based batch scheduling
- âœ… Gradient accumulation (4 steps)
- âœ… Efficient memory usage
- âœ… Optimized for 4GB VRAM GPUs
- âœ… Pre-padded sequences for faster training

## ğŸ”§ Training Configuration
- **Batch Size:** 4 (16 with gradient accumulation)
- **Learning Rate:** 3e-4 with cosine decay
- **Weight Decay:** 0.1
- **Training Data:** Wikitext-2
- **Epochs:** 3

## ğŸ“Š Memory Usage
- **GPU VRAM:** ~3.5GB peak
- **Recommended GPU:** 4GB+ VRAM
- **CPU RAM:** ~8GB recommended

## ğŸ“œ License
This project is licensed under the MIT License.

---
ğŸš€ Happy Training! Feel free to contribute or raise issues. ğŸ¯


